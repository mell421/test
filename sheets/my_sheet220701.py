# -*- coding: utf-8 -*-
"""my sheet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E5EbGv0EURcTWWJslMNvKkrGJ8Rn3DGd

load
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from wordcloud import WordCloud
import string

aAccueil_url = "https://docs.google.com/spreadsheets/d/1HZydd6qo-GcQVqnwQnerHHnMEyR6faJXKN7Egkuqp4I/edit#gid=1775525986"
aAccueil = aAccueil_url.replace('/edit#gid=', '/export?format=csv&gid=')
#Accueil = pd.read_csv(aAccueil)

aCopyScript_url = "https://docs.google.com/spreadsheets/d/1HZydd6qo-GcQVqnwQnerHHnMEyR6faJXKN7Egkuqp4I/edit#gid=1292069466"
aCopyScript = aCopyScript_url.replace('/edit#gid=', '/export?format=csv&gid=')

aPARS_url = "https://docs.google.com/spreadsheets/d/1HZydd6qo-GcQVqnwQnerHHnMEyR6faJXKN7Egkuqp4I/edit#gid=499443320"
aPARS = aPARS_url.replace('/edit#gid=', '/export?format=csv&gid=')

aPARS1_url = "https://docs.google.com/spreadsheets/d/1HZydd6qo-GcQVqnwQnerHHnMEyR6faJXKN7Egkuqp4I/edit#gid=357008918"
aPARS1 = aPARS1_url.replace('/edit#gid=', '/export?format=csv&gid=')

"""ACCUEIL"""

Accueil = pd.read_csv(aAccueil)
print(Accueil)

import numpy as np

Accueil[['Horodateur','Titre','saison','episode','status','tisa','tisaep']].to_csv("ajout.csv")
print(Accueil.columns)
print(Accueil.describe())
print("---")
val = Accueil.min()
print(val)
print("---")
val = Accueil.mean()
print(val)
print("---")
#moyenne
print("Moyenne saison    "+str(Accueil['saison'].mean()))
#print("Moyenne saison recent "+str(Accueil['saison.1'].mean()))
#print("Moyenne indice    "+str(Accueil['indice'].mean()))
print("---")
print("Moyenne hh        "+str(Accueil['hh'].mean()))
print("---")
#print("Moyenne mm recent "+str(Accueil['M'].mean()))
#print("diff mm-mr        "+str(Accueil['M'].mean()-Accueil['minute'].mean()))



print(Accueil['Titre'].nunique())

mat = pd.crosstab(Accueil['saison'],Accueil['episode'])
print(mat)

t=Accueil['Titre'].value_counts().to_frame()
t

s=Accueil['saison'].value_counts().to_frame()
s

e=Accueil['episode'].value_counts().to_frame()
e

Accueil.groupby(['status']).count().plot.pie(y='Horodateur',figsize=(5, 5),autopct='%1.1f%%', startangle=90,title='all')
Accueil.groupby(['status']).count().plot(kind='bar', y='Horodateur',title='all')

Accueil.groupby(['saison']).count().plot(kind='bar', y='Titre',title='saison')
Accueil.groupby(['hh']).count().plot(kind='bar', y='Titre',title='hh')

import re
import os
Accueil['Titre_pp']=Accueil['Titre'].map(lambda x: re.sub('[,\.!?]', '', x))
Accueil['Titre_pp']=Accueil['Titre_pp'].map(lambda x: x.lower())
Accueil['Titre_pp'].head()

# Import the wordcloud library
from wordcloud import WordCloud
# Join the different processed titles together.
long_string = ','.join(list(Accueil['Titre_pp'].values))
# Create a WordCloud object
wordcloud = WordCloud(background_color="white", max_words=5000, contour_width=3, contour_color='steelblue')
# Generate a word cloud
wordcloud.generate(long_string)
# Visualize the word cloud
wordcloud.to_image()

import gensim
from gensim.utils import simple_preprocess
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
#stop_words = stopwords.words('english')
#stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
def sent_to_words(sentences):
    for sentence in sentences:
        # deacc=True removes punctuations
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) 
             if word not in stop_words] for doc in texts]
data = Accueil.Titre_pp.values.tolist()
data_words = list(sent_to_words(data))
# remove stop words
#data_words = remove_stopwords(data_words)
print(data_words[:1][0][:30])

import gensim.corpora as corpora
# Create Dictionary
id2word = corpora.Dictionary(data_words)
# Create Corpus
texts = data_words
# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]
# View
print(corpus[:1][0][:30])

from pprint import pprint
# number of topics
num_topics = 4
# Build LDA model
lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                       id2word=id2word,
                                       num_topics=num_topics)
# Print the Keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

!pip install pyLDAvis
import pyLDAvis.gensim_models
import pickle 
import pyLDAvis
# Visualize the topics
pyLDAvis.enable_notebook()
LDAvis_data_filepath = os.path.join('./lda_'+str(num_topics))
# # this is a bit time consuming - make the if statement True
# # if you want to execute visualization prep yourself
if 1 == 1:
    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)
    with open(LDAvis_data_filepath, 'wb') as f:
        pickle.dump(LDAvis_prepared, f)
# load the pre-prepared pyLDAvis data from disk
with open(LDAvis_data_filepath, 'rb') as f:
    LDAvis_prepared = pickle.load(f)
pyLDAvis.save_html(LDAvis_prepared, './lda_'+ str(num_topics) +'.html')
LDAvis_prepared

import string
text = str(Accueil['Titre'].tolist()).replace('\'',' ').replace('\n',' ').replace(',',' ').replace('\r\n', ' ').replace('  ', ' ').replace('   ', ' ')
print(text)
wordcloud = WordCloud().generate(text)
plt.imshow(wordcloud)
plt.axis("off")
plt.show();

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer()
tfidf.fit(Accueil['Titre'])
X = tfidf.transform(Accueil['Titre'])
print([X[1, tfidf.vocabulary_['world']]])
print([X[1, tfidf.vocabulary_['of']]])
print([X[1, tfidf.vocabulary_['dance']]])

import numpy as np
nvAccueil=Accueil[['Titre','episode','status']]
nvAccueil.head()

from sklearn.model_selection import train_test_split
X = nvAccueil.episode
y = nvAccueil.status
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)
print("Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive".format(len(X_train),
                                                                             (len(X_train[y_train == 0]) / (len(X_train)*1.))*100,
                                                                            (len(X_train[y_train == 1]) / (len(X_train)*1.))*100))
print("Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive".format(len(X_test),
                                                                             (len(X_test[y_test == 0]) / (len(X_test)*1.))*100,
                                                                            (len(X_test[y_test == 1]) / (len(X_test)*1.))*100))

"""COPY SCRIPT"""

CopyScript = pd.read_csv(aCopyScript)
print(CopyScript)

print(CopyScript.columns)
print("---")
val = CopyScript.min()
print(val)
print("---")
val = CopyScript.mean()
print(val)
print("---")
print("Moyenne nbCarConclu    "+str(CopyScript['nbCarConclu'].mean()))
print("Moyenne nbCarConcluBis "+str(CopyScript['nbCarConcluBis'].mean()))

CopyScript.groupby(['nc']).count().plot(kind='bar', y='dateConc',title='nc')
CopyScript.groupby(['ncb']).count().plot(kind='bar', y='dateConc',title='ncb')
CopyScript.groupby(['nbCarConclu']).count().plot(kind='bar', y='dateConc',title='nbCarConclu')
CopyScript.groupby(['nbCarConcluBis']).count().plot(kind='bar', y='dateConc',title='nbCarConcluBis')

CopyScript.groupby(['tauxSem']).count().plot(kind='bar', y='dateSem',title='taux sem')
CopyScript.groupby(['nrs']).count().plot(kind='bar', y='dateSem',title='nb sem')
CopyScript.groupby(['tauxMois']).count().plot(kind='bar', y='dateMois',title='taux mois')
CopyScript.groupby(['nrm']).count().plot(kind='bar', y='dateMois',title='nb mois')

"""PARS"""

PARS = pd.read_csv(aPARS)
print(PARS)

val = PARS.mean()
print(val)

PARS.groupby(['conclu']).count().plot(kind='bar', y='date',title='conclu')
PARS.groupby(['conclubis']).count().plot(kind='bar', y='date',title='conclubis')
PARS.groupby(['max']).count().plot(kind='bar', y='date',title='max')
PARS.groupby(['sem']).count().plot(kind='bar', y='date',title='sem')
PARS.groupby(['mois']).count().plot(kind='bar', y='date',title='mois')
PARS.groupby(['eff']).count().plot(kind='bar', y='date',title='eff')
PARS.groupby(['eff7']).count().plot(kind='bar', y='date',title='eff7')
PARS.groupby(['eff6r']).count().plot(kind='bar', y='date',title='eff6r')
PARS.groupby(['premderall']).count().plot(kind='bar', y='date',title='premderall')
PARS.groupby(['last48tri']).count().plot(kind='bar', y='date',title='last48tri')
PARS.groupby(['last46']).count().plot(kind='bar', y='date',title='last46')
PARS.groupby(['offset3']).count().plot(kind='bar', y='date',title='offset3')
PARS.groupby(['nbPageTri']).count().plot(kind='bar', y='date',title='nbPageTri')
PARS.groupby(['nbPage']).count().plot(kind='bar', y='date',title='nbPage')
PARS.groupby(['nbTpsPc']).count().plot(kind='bar', y='date',title='nbTpsPc')

"""PARS2"""

PARS1 = pd.read_csv(aPARS1)
print(PARS1)

val = PARS1.mean()
print(val)

PARS1.groupby(['j']).count().plot(kind='bar', y='date',title='j')
PARS1.groupby(['s']).count().plot(kind='bar', y='date',title='s')
PARS1.groupby(['m']).count().plot(kind='bar', y='date',title='m')