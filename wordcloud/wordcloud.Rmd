---
title: "Wordcloud"
output:
  html_document:
    df_print: paged
    toc: false
---
```{r setup, include = FALSE}
suppressWarnings(source("~/GitHub/ajoutRep/ajoutRep/R/sources.R"))
suppressWarnings(library(tidyverse))
# knitr::opts_chunk$set(cache = TRUE)
library(ggraph)
library(tidygraph)
suppressWarnings(library(tidyverse))
library(dplyr)
require(devtools)
library(wordcloud2)
library(car)

```


[retour](./../index.html)  

<!-- -->
[MD](./wordcloud.Rmd)  


## accueil
```{r accueil, echo=FALSE}
text <- data.frame(accueil())
text <- text[2]

tm <- function(text){
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeAccentE <- content_transformer(function (x , pattern ) gsub(pattern, "e", x))
  removeAccentA <- content_transformer(function (x , pattern ) gsub(pattern, "a", x))
  removeC <- content_transformer(function (x , pattern ) gsub(pattern, "c", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  # TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  TextDoc <- tm_map(TextDoc, toSpace, "'<'")
  TextDoc <- tm_map(TextDoc, toSpace, "\\'")
  TextDoc <- tm_map(TextDoc, removeAccentE, c("é","è","ê"))
  TextDoc <- tm_map(TextDoc, removeAccentA, c("à","â"))
  TextDoc <- tm_map(TextDoc, removeC, c("ç"))
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","the"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 accueil",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  
  dev.print(device = png, file = "accueil_2.png", width = 600)
  
    wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")
    
  
}

suppressWarnings(tm(text))
```

## en cours
```{r encours, echo=FALSE}
text <- data.frame(listDesc.ec())
text <- text[2]

tm <- function(text){
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeAccentE <- content_transformer(function (x , pattern ) gsub(pattern, "e", x))
  removeAccentA <- content_transformer(function (x , pattern ) gsub(pattern, "a", x))
  removeC <- content_transformer(function (x , pattern ) gsub(pattern, "c", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  # TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  TextDoc <- tm_map(TextDoc, toSpace, "'<'")
  TextDoc <- tm_map(TextDoc, toSpace, "\\'")
  TextDoc <- tm_map(TextDoc, removeAccentE, c("é","è","ê"))
  TextDoc <- tm_map(TextDoc, removeAccentA, c("à","â"))
  TextDoc <- tm_map(TextDoc, removeC, c("ç"))
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","the"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 en cours",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  
  dev.print(device = png, file = "encours_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  # 
}

suppressWarnings(tm(text))
```

## termine
```{r termine, echo=FALSE}
text <- data.frame(listDesc.ter())
text <- text[2]

tm <- function(text){
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeAccentE <- content_transformer(function (x , pattern ) gsub(pattern, "e", x))
  removeAccentA <- content_transformer(function (x , pattern ) gsub(pattern, "a", x))
  removeC <- content_transformer(function (x , pattern ) gsub(pattern, "c", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  # TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  TextDoc <- tm_map(TextDoc, toSpace, "'<'")
  TextDoc <- tm_map(TextDoc, toSpace, "\\'")
  # TextDoc <- tm_map(TextDoc, removeAccentE, c("é","è","ê"))
  # TextDoc <- tm_map(TextDoc, removeAccentA, c("à","â"))
  # TextDoc <- tm_map(TextDoc, removeC, c("ç"))
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","the"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 termine",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  
  dev.print(device = png, file = "termine_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  # 
}

suppressWarnings(tm(text))
```

## copy
```{r copy, echo=FALSE}

main <- function(){
  text <- data.frame(copy())
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","the"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v) ,freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 copy",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  
  dev.print(device = png, file = "copy_2.png", width=600)

    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

}

suppressWarnings(main())
```

## conclu
```{r conclu, echo=FALSE}

main <- function(){
  text2 <- data.frame(aTestConclu())
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis","the", "eff", "na", "conclucompi", "conclucompibis"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 conclu",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  
  dev.print(device = png, file = "conclu_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  

}

suppressWarnings(main())
```

## conclubis
```{r conclubis, echo=FALSE}

main <- function(){
  text2 <- data.frame(aTestConcluBis())
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis","the", "eff", "na", "conclucompi", "conclucompibis"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 conclubis",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "conclubis_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  

}

suppressWarnings(main())
```

## sem
```{r sem, echo=FALSE}

main <- function(){
  text2 <- data.frame(aTestSem())
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis","the", "eff", "na", "conclucompi", "conclucompibis","sem"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 sem",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
dev.print(device = png, file = "sem_2.png", width = 600)

      # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  

}

suppressWarnings(main())
```

## mois
```{r mois, echo=FALSE}

main <- function(){
  text2 <- data.frame(aTestMois())
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis","the", "eff", "na", "conclucompi", "conclucompibis","mois"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 mois",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "mois_2.png", width=600)
  
    # wordcloud2(data = dtm_d, color = "random-light",shape="circle",backgroundColor ="black")

  # 

}

suppressWarnings(main())
```

## max
```{r max, echo=FALSE}

main <- function(){
  text2 <- copy.max()
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","na","the","conclucompi","conclucompibis"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(
    dtm_d[1:20,]$freq, 
    las = 2, 
    names.arg = dtm_d[1:20,]$word,
    col =brewer.pal(8, "Dark2"), 
    main ="Top 20 max",
    ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "max_2.png", width = 600)
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  

}

suppressWarnings(main())
```

## eff
```{r eff, echo=FALSE}

main <- function(){
  text2 <- aEff()
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","na","the","conclucompi","conclucompibis"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(
    dtm_d[1:20,]$freq, 
    las = 2, 
    names.arg = dtm_d[1:20,]$word,
    col =brewer.pal(8, "Dark2"), 
    main ="Top 20 eff",
    ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "eff_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  # 

}

suppressWarnings(main())
```

## resume
```{r resume, echo=FALSE}

main <- function(){
  text2 <- data.frame(aTestResume())
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis","the", "eff","na","conclucompi","conclucompibis"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:20,]$freq, las = 2, names.arg = dtm_d[1:20,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 20 resume",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,max.words=1500, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "resume_2.png", width = 600)
  
    # wordcloud2(data = dtm_d, color = "random-light",backgroundColor ="black")

  # 

}

suppressWarnings(main())
```

---

## historique
```{r hist, echo=FALSE}
data <- data.frame(listDesc.desc())
data %>% select(tisaep,Horodateur,status) 
```

## debut
```{r lettre, echo=FALSE}
data <- data.frame(listDesc.desc())
data %>% filter(episode == 1,status=="en cours") %>% select(tisaep,Horodateur,descr.img) 

```

## liste EC : alea
```{r alea, echo=FALSE}

data <- data.frame(listDesc.desc())
max <- 100
data %>% filter(status == "en cours") %>% sample_n(max) %>% group_by(tisa) %>% summarise(pc = n()) %>% arrange(desc(pc)) %>% filter(pc >= 3)
```

## liste EC : Titre
```{r titre, echo=FALSE}
df <- data.frame(listDesc.ec())
data <- data.frame(listDesc.desc())
data <- distinct(data)
# titre = readline()
titre = df[1,2]
data %>% filter(Titre == titre) %>% select(tisaep,Horodateur) 
if(data[2,2] == data[1,2]){
  titre = df[3,2]
} else {
  titre = df[2,2]
}
data %>% filter(Titre == titre) %>% select(tisaep,Horodateur) 
```

## liste EC
```{r ec, echo=FALSE}
data <- data.frame(listDesc.desc())
data %>% filter(status == "en cours") %>% select(tisaep,Horodateur) 
```

## liste TER
```{r ter, echo=FALSE}
data <- data.frame(listDesc.desc())
data %>% filter(status == "terminée") %>% select(tisaep,Horodateur) 
```

## liste film
```{r film, echo=FALSE}
data <- data.frame(listDesc.desc())
data %>% filter(status == "film") %>% select(tisaep,Horodateur) 
```

## premder ec
```{r premDerEc, echo=FALSE}
data <- data.frame(premder.ec())
data %>% filter(tisaep.all_2 != "")  %>% select(tisaep.all_2
,date.premder.all_2) 
```

## premder
```{r premder, echo=FALSE}
data <- data.frame(premder.premder())
data %>% filter(tisaep.all_1 != "") %>% select(tisaep.all_1,date.premder.all_1,status.all_1) 
```

## prem der
```{r premDer, echo=FALSE}
data <- data.frame(premder.prem.der())
data %>% filter(tisaep.all != "")  %>% select(tisaep.all
,date.premder.all,status.all) 
```

## nb
```{r nb, echo=FALSE}
data <- data.frame(aListes())

# df <- data.frame(cbind(data$titre,data$nbDeLignes,data$titre_1,data$nbPage))
# df[1:50,]

data %>% filter(titre != "")  %>% select(titre,nbDeLignes) 
data %>% filter(titre_1 != "")  %>% select(titre_1,nbPage) 
```
---

## conclu
```{r concluG, echo = FALSE}
CopyS <-data.frame(copy.conclu())
dataC <- data.frame(CopyS$nbCarConclu,CopyS$nbCarConcluBis,CopyS$jour_1,CopyS$date)
  
  plot(CopyS$nbCarConcluBis,main="nb de caracteres conclu",type = "l",col="red",xlab = "date",ylab = "nb car")
  lines(CopyS$nbCarConcluBis,type = "l",col="red")
  lines(CopyS$nbCarConclu,type = "l",col="blue")
  
```

## max
```{r max2, echo = FALSE}
CopyS <- data.frame(copy.max())
main <- function(){
# CopyS <- data.frame()
  CopyS <- na.omit(CopyS)
  plot(CopyS$NBog,main="max",type = "l",col="red",xlab = "date",ylab = "nb")
  lines(CopyS$NBog,type = "l",col="red")
  lines(CopyS$NBactuel,type = "l",col="blue")
}
suppressWarnings(main())
```

## regression max
```{r regress0, echo = FALSE}
# install.packages("car")
Prestige <- copy.max()
    Prestige <- as_tibble(Prestige)

    # as.tibble(Prestige)
     layout(matrix(1:4,2,2))
    # 1
scatterplot(NBog~NBactuel
, data=Prestige)
# 2
# scatterplot(nc~ncb, data=Prestige)
# 3
# scatterplot(nbCarConclu~nc, data=Prestige)
# 4
# scatterplot(nbCarConcluBis~ncb, data=Prestige)

prest.lm1 <- lm(NBog~NBactuel, data=Prestige)
acf(residuals(prest.lm1), main="prest.lm1")
# durbinWatsonTest (prest.lm1)
plot(prest.lm1,2)
# shapiro.test(residuals(prest.lm1))
plot(prest.lm1, 3)
# ncvTest(prest.lm1)
plot(prest.lm1,1)
# influenceIndexPlot(prest.lm1)
```

## regression1 nbCarConclu~nbCarConcluBis
```{r regress1, echo = FALSE}
# install.packages("car")
Prestige <- copy.conclu()
    Prestige <- as_tibble(Prestige)

    # as.tibble(Prestige)
     layout(matrix(1:4,2,2))
    # 1
scatterplot(nbCarConclu~nbCarConcluBis
, data=Prestige)
# 2
# scatterplot(nc~ncb, data=Prestige)
# 3
# scatterplot(nbCarConclu~nc, data=Prestige)
# 4
# scatterplot(nbCarConcluBis~ncb, data=Prestige)

prest.lm1 <- lm(nbCarConclu~nbCarConcluBis, data=Prestige)
acf(residuals(prest.lm1), main="prest.lm1")
# durbinWatsonTest (prest.lm1)
plot(prest.lm1,2)
# shapiro.test(residuals(prest.lm1))
plot(prest.lm1, 3)
# ncvTest(prest.lm1)
plot(prest.lm1,1)
# influenceIndexPlot(prest.lm1)
```

## regression2 nc~ncb
```{r regress2, echo = FALSE}
# install.packages("car")
Prestige <- copy.conclu()
    Prestige <- as_tibble(Prestige)

    # as.tibble(Prestige)
    layout(matrix(1:4,2,2))
# 2
scatterplot(nc~ncb
, data=Prestige)

prest.lm1 <- lm(nc~ncb, data=Prestige)
acf(residuals(prest.lm1), main="prest.lm1")
# durbinWatsonTest (prest.lm1)
plot(prest.lm1,2)
# shapiro.test(residuals(prest.lm1))
plot(prest.lm1, 3)
# ncvTest(prest.lm1)
plot(prest.lm1,1)
```

## regression3 nbCarConclu~nc
```{r regress3, echo = FALSE}
# install.packages("car")
Prestige <- copy.conclu()
    Prestige <- as_tibble(Prestige)

    # as.tibble(Prestige)
    layout(matrix(1:4,2,2))
# 3
scatterplot(nbCarConclu~nc
, data=Prestige)

prest.lm1 <- lm(nbCarConclu~nc, data=Prestige)
acf(residuals(prest.lm1), main="prest.lm1")
# durbinWatsonTest (prest.lm1)
plot(prest.lm1,2)
# shapiro.test(residuals(prest.lm1))
plot(prest.lm1, 3)
# ncvTest(prest.lm1)
plot(prest.lm1,1)
```

## regression4 nbCarConcluBis~ncb
```{r regress4, echo = FALSE}
# install.packages("car")
Prestige <- copy.conclu()
    Prestige <- as_tibble(Prestige)

    # as.tibble(Prestige)
  layout(matrix(1:4,2,2))
# 4
scatterplot(nbCarConcluBis~ncb
, data=Prestige)

prest.lm1 <- lm(nbCarConcluBis~ncb, data=Prestige)
acf(residuals(prest.lm1), main="prest.lm1")
# durbinWatsonTest (prest.lm1)
plot(prest.lm1,2)
# shapiro.test(residuals(prest.lm1))
plot(prest.lm1, 3)
# ncvTest(prest.lm1)
plot(prest.lm1,1)
```
---
## en cours
```{r ecocc, echo=FALSE}
CopyS <- data.frame(aECocc())
CopyS <- data.frame(CopyS[1:6])
main <- function(){
  max <- 30
# CopyS <- data.frame()
  CopyS <- na.omit(CopyS)
  print("semaine")
  sem <- CopyS %>% filter(occ.ECS >= 1)
  sem2 <- sem %>% arrange(desc(sem$occ.ECS))
  barplot(sem$occ.ECS, 
          las = 2, 
          names.arg = sem$en.cours,
          col =brewer.pal(8, "Dark2"), 
          main ="Top semaine",
          ylab = "Word frequencies",
          ylim=c(0,max(sem$occ.ECS)*1.1))
  box()
  pie(sem2$occ.ECS,label=paste(sem2$en.cours,"",sem2$occ.ECS))
  print("mois")
  mois <- CopyS %>% filter(occ.ECM >= 1)
  mois2 <- mois %>% arrange(desc(mois$occ.ECM))
  barplot(mois$occ.ECM, 
          las = 2, 
          names.arg = mois$en.cours,
          col =brewer.pal(8, "Dark2"), 
          main ="Top mois",
          ylab = "Word frequencies",
          ylim=c(0,max(mois$occ.ECM)*1.1))
  box()
  pie(mois2$occ.ECM,label=paste(mois2$en.cours,"",mois2$occ.ECM))
  print("recent")
  rec <- CopyS %>% filter(occ.ECR >= 1)
  rec2 <- rec %>% arrange(desc(rec$occ.ECR))
  barplot(rec$occ.ECR, 
          las = 2, 
          names.arg = rec$en.cours,
          col =brewer.pal(8, "Dark2"), 
          main ="Top recent",
          ylab = "Word frequencies",
          ylim=c(0,max(rec$occ.ECR)*1.1))
  box()
  pie(rec2$occ.ECR,label=paste(rec2$en.cours,"",rec2$occ.ECR))
  print("tout")
  barplot(CopyS$count, 
          las = 2, 
          names.arg = CopyS$tisa.en.cours,
          col =brewer.pal(8, "Dark2"), 
          main ="Top all",
          ylab = "Word frequencies",
          ylim=c(0,max(CopyS$count)*1.1))
  box()
  print("---")
  barplot(t(as.matrix(CopyS[2:4])), 
          las = 2, 
          names.arg = CopyS$en.cours,
          col =c("red","blue","green"), 
          main ="Top ---",
          ylab = "Word frequencies",
          beside = TRUE,
          ylim=c(0,max(CopyS$occ.ECR)*1.1))
  box()
  legend('topright',fill=c("red","blue","green"),legend=c('recent','mois','sem'))
}
suppressWarnings(main())
```

## termine
```{r terocc, echo=FALSE}
CopyS <- data.frame(aECocc())
CopyS <- data.frame(CopyS[7:12])
main <- function(){
  max <- 30
# CopyS <- data.frame()
  CopyS <- na.omit(CopyS)
  print("semaine")
  sem <- CopyS %>% filter(occ.FINIS >= 1)
  barplot(sem$occ.FINIS, 
          las = 2, 
          names.arg = sem$fini,
          col =brewer.pal(8, "Dark2"), 
          main ="Top semaine",
          ylab = "Word frequencies",
          ylim=c(0,max(sem$occ.FINIS)*1.1))
  box()
  print("mois")
  mois <- CopyS %>% filter(occ.FINIM >= 1)
  barplot(mois$occ.FINIM, 
          las = 2, 
          names.arg = mois$fini,
          col =brewer.pal(8, "Dark2"), 
          main ="Top mois",
          ylab = "Word frequencies",
          ylim=c(0,max(mois$occ.FINIM)*1.1))
  box()
  print("recent")
  barplot(CopyS$occ.FINIR, 
          las = 2, 
          names.arg = CopyS$fini,
          col =brewer.pal(8, "Dark2"), 
          main ="Top recent",
          ylab = "Word frequencies",
          ylim=c(0,max(CopyS$occ.FINIR)*1.1))
  box()
  print("tout")
  barplot(CopyS$count_1, 
          las = 2, 
          names.arg = CopyS$tisa.terminée,
          col =brewer.pal(8, "Dark2"), 
          main ="Top all",
          ylab = "Word frequencies",
          ylim=c(0,max(CopyS$count_1)*1.1))
  box()
  print("---")
  barplot(t(as.matrix(CopyS[2:4])), 
          las = 2, 
          names.arg = CopyS$fini,
          col =c("red","blue","green"), 
          main ="Top ---",
          ylab = "Word frequencies",
          beside = TRUE,
          ylim=c(0,max(CopyS$occ.FINIR)*1.1))
  box()
  legend('topright',fill=c("red","blue","green"),legend=c('recent','mois','sem'))
}
suppressWarnings(main())
```

## temps
```{r temps, echo=FALSE}

df <- data.frame(accueil())
layout(matrix(1:4,2,2))

# print("heures")
barplot(table(df$hh),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="ep per hours",
        ylab = "nb ep",
        ylim=c(0,max(table(df$hh))*1.1)
        )
box()
# print("minutes")
barplot(table(df$minute),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="ep per minutes",
        ylab = "nb ep",
        ylim=c(0,max(table(df$minute))*1.1)
        )
box()

# print("secondes")
barplot(table(df$sec),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="ep per seconds",
        ylab = "nb ep",
        ylim=c(0,max(table(df$sec))*1.1)
        )
box()
```

## accueil
```{r acc, echo=FALSE}

df <- data.frame(accueil())
layout(matrix(1:4,2,2))
# print("saison")
barplot(table(df$saison),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="saison",
        ylab = "nb ep",
        ylim=c(0,max(table(df$saison))*1.1)
        )
box()
# print("episode")
barplot(table(df$episode),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="episode",
        ylab = "nb ep",
        ylim=c(0,max(table(df$episode))*1.1)
        )
box()
# print("status")
barplot(table(df$status),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="status",
        ylab = "nb ep",
        ylim=c(0,max(table(df$status))*1.1)
        )
box()
# print("indice")
barplot(table(df$indice),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="indice",
        ylab = "nb ep",
        ylim=c(0,max(table(df$indice))*1.1)
        )
box()

```

## copy
```{r copy2, echo=FALSE}

df <- data.frame(aCopyScript())
layout(matrix(1:4,2,2))

# print("nb titre sem")
barplot(table(df$nrs),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="nb titre sem",
        ylab = "nb sem with x titles",
        ylim=c(0,max(table(df$nrs))*1.1)
        )
box()
# print("nb caractere sem")
barplot(table(df$nbCar),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="nb caractere sem",
        ylab = "nb sem with x car",
        ylim=c(0,max(table(df$nbCar))*1.1)
        )
box()
# print("nb titre mois")
barplot(table(df$nrm),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="nb titre mois",
        ylab = "nb mois with x titles",
        ylim=c(0,max(table(df$nrm))*1.1)
        )
box()

# print("nb caractere mois")
barplot(table(df$nbCar_1),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="nb caractere mois",
        ylab = "nb mois with x car",
        ylim=c(0,max(table(df$nbCar_1))*1.1)
        )
box()
layout(matrix(1:2,1,2))
# print("nb titre conclu")
barplot(table(df$nc),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="nb titre conclu",
        ylab = "nc",
        ylim=c(0,max(table(df$nc))*1.1)
        )
box()
# print("nb titre conclubis")
barplot(table(df$ncb),
        las = 2,
        col =brewer.pal(8, "Dark2"), 
        main ="nb titre conclubis",
        ylab = "ncb",
        ylim=c(0,max(table(df$ncb))*1.1)
        )
box()
```
---

## networks
```{r network1, echo=FALSE}

netw <- function(max=15){
  
df <- data.frame(listeCompl.ec())
df <- df[1:max,]

  ### CREATE NODES TABLE 
get_random_names <- function(n) { 
  unique_names <- distinct(df, en.cours, .keep_all = TRUE) 
  index <- sample(1:nrow(unique_names), n, replace = FALSE) 
  names <- unique_names[index, ] 
  names 
}

nodes <- get_random_names(max)


### CREATE LINKS TABLE 
# Create source and target vectors 
src <- sample(1:nrow(nodes), nrow(nodes)*2, replace = TRUE)  
target <- sample(1:nrow(nodes), nrow(nodes)*2, replace = TRUE)

# Merge vectors to form a single table 
links <- data.frame(src, target)

# Clean up 
links <- data.frame(src, target) %>%  
  filter(!(src == target)) 
links <- unique(links[c("src", "target")])


### PLOT NETWORK 
# Type cast to tbl_graph object
social_net_tbls <- tbl_graph(nodes = nodes, 
                             edges = links, 
                             directed = FALSE)

# Create the network 
social_net <- ggraph(social_net_tbls, layout = "stress") +
  geom_node_point(size = 2) +                                         
  geom_node_text(aes(label = en.cours), nudge_y = 0.05, nudge_x = 0.2)+ 
  geom_edge_link() +
  theme_void()

# Render the network 
  show(social_net)
  # 


}

netw(10) 
dev.print(device = png, file = "network1_1.png", width = 600)
netw(10)
dev.print(device = png, file = "network1_2.png", width = 600)
```
