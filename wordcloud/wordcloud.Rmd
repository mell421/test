---
title: "Wordcloud"
output:
  html_document:
    df_print: paged
---
```{r setup, include = FALSE}
suppressWarnings(source("~/GitHub/ajoutRep/ajoutRep/R/sources.R"))
suppressWarnings(library(tidyverse))
library(ggraph)
library(tidygraph)
require(devtools)
library(wordcloud2)
```

```{r render, eval=FALSE, include=FALSE}
rmarkdown::render_site()
```

[retour](./../index.html)  

<!-- -->
[MD](./wordcloud.Rmd)  

## menu
- [menu](#menu)
- [accueil](#accueil)
- [page copy](#page-copy)
- [copy max](#copy-max)
- [test resume](#test-resume)
- [debut](#debut)
- [liste EC : alea](#liste-ec--alea)
- [liste EC : Titre](#liste-ec--titre)
- [liste EC](#liste-ec)
- [liste TER](#liste-ter)
- [liste film](#liste-film)
- [premder ec](#premder-ec)
- [premder](#premder)
- [prem der](#prem-der)
- [nb](#nb)
- [networks](#networks)
- [network2](#network2)





## accueil
[haut](#menu)
```{r accueil, echo=FALSE}
text <- data.frame(accueil())
text <- text[2]

tm <- function(text){
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","the"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:15,]$freq, las = 2, names.arg = dtm_d[1:15,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 15 accueil",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,
            max.words=150, random.order=FALSE, rot.per=0.35,
            colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "accueil.png", width = 600)
}

suppressWarnings(tm(text))
```

## page copy
[haut](#menu)
```{r copy, echo=FALSE}

main <- function(){
  text <- data.frame(copy())
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","the"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v) ,freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:15,]$freq, las = 2, names.arg = dtm_d[1:15,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 15 copy",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,
            max.words=150, random.order=FALSE, rot.per=0.35,
            colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "copy.png", width = 600)
  wordcloud2(data = dtm_d, color = "random-light",shape="circle",backgroundColor = "black")

}

suppressWarnings(main())
```

## copy max
[haut](#menu)
```{r copyMax, echo=FALSE}

main <- function(){
  text2 <- copy.max()
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis", "eff","na","the","conclucompi","conclucompibis"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(
    dtm_d[1:15,]$freq, 
    las = 2, 
    names.arg = dtm_d[1:15,]$word,
    col =brewer.pal(8, "Dark2"), 
    main ="Top 15 max",
    ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,
            max.words=200, random.order=FALSE, rot.per=0.35,
            colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "max.png", width = 600)
  wordcloud2(data = dtm_d, color = "random-light",shape="circle",backgroundColor = "black")

}

suppressWarnings(main())
```

## test resume
[haut](#menu)
```{r resume, echo=FALSE}

main <- function(){
  text2 <- data.frame(aTestResume())
  text2 <- text2[2]
  # Read the text file from local machine , choose file interactively
  # text2 <- readLines(file.choose())
  # Load the data as a corpus
  TextDoc <- Corpus(VectorSource(text2))

  #Replacing "/", "@" and "|" with space
  toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
  removeSpace <- content_transformer(function (x , pattern ) gsub(pattern, "", x))
  TextDoc <- tm_map(TextDoc, toSpace, "/")
  TextDoc <- tm_map(TextDoc, toSpace, "@")
  TextDoc <- tm_map(TextDoc, toSpace, "\\|")
  # Convert the text to lower case
  TextDoc <- tm_map(TextDoc, content_transformer(tolower))
  # Remove numbers
  # TextDoc <- tm_map(TextDoc, removeNumbers)
  # Remove english common stopwords
  # TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
  # Remove your own stop word
  # specify your custom stopwords as a character vector
  TextDoc <- tm_map(TextDoc, removeWords, c("conclu", "conclubis","the", "eff","na","conclucompi","conclucompibis"))
  # Remove punctuations
  TextDoc <- tm_map(TextDoc, removePunctuation)
  # Eliminate extra white spaces
  TextDoc <- tm_map(TextDoc, stripWhitespace)
  # Eliminate spaces
  # TextDoc <- gsub("[[:blank:]]", "", TextDoc)
  # Text stemming - which reduces words to their root form
  # TextDoc <- tm_map(TextDoc, stemDocument)

  # Build a term-document matrix
  TextDoc_dtm <- TermDocumentMatrix(TextDoc)
  dtm_m <- as.matrix(TextDoc_dtm)
  # Sort by descearing value of frequency
  dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
  dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
  # Display the top 20 most frequent words
  head(dtm_d, 30)

  # Plot the most frequent words
  barplot(dtm_d[1:15,]$freq, las = 2, names.arg = dtm_d[1:15,]$word,
          col =brewer.pal(8, "Dark2"), main ="Top 15 resume",
          ylab = "Word frequencies")

  #generate word cloud
  # set.seed(1234)
  wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 1,
            max.words=200, random.order=FALSE, rot.per=0.35,
            colors=brewer.pal(8, "Dark2"))
  dev.print(device = png, file = "ec.png", width = 600)
  wordcloud2(data = dtm_d, color = "random-light",shape="circle",backgroundColor = "black")

}

suppressWarnings(main())
```

## debut
[haut](#menu)
```{r lettre, echo=FALSE}
data <- data.frame(listDesc.desc())
data %>% filter(episode == 1,status=="en cours") %>% select(tisaep,Horodateur,descr.img) 

```

## liste EC : alea
[haut](#menu)
```{r alea, echo=FALSE}
data <- data.frame(listDesc.desc())
max <- 100
data %>% filter(status == "en cours") %>% sample_n(max) %>% group_by(tisa) %>% summarise(pc = n()) %>% arrange(desc(pc)) %>% filter(pc >= 2)
```

## liste EC : Titre
[haut](#menu)
```{r titre, echo=FALSE}
df <- data.frame(listDesc.ec())
data <- data.frame(listDesc.desc())
data <- distinct(data)
# titre = readline()
titre = df[1,2]
data %>% filter(Titre == titre) %>% select(tisaep,Horodateur) 
if(data[2,2] == data[1,2]){
  titre = df[3,2]
} else {
  titre = df[2,2]
}
data %>% filter(Titre == titre) %>% select(tisaep,Horodateur) 
```

## liste EC
[haut](#menu)
```{r ec, echo=FALSE}
data <- data.frame(listDesc.desc())
data %>% filter(status == "en cours") %>% select(tisaep,Horodateur) 
```

## liste TER
[haut](#menu)
```{r ter, echo=FALSE}
data <- data.frame(listDesc.desc())
data %>% filter(status == "terminée") %>% select(tisaep,Horodateur) 
```

## liste film
[haut](#menu)
```{r film, echo=FALSE}
data <- data.frame(listDesc.desc())
data %>% filter(status == "film") %>% select(tisaep,Horodateur) 
```

## premder ec
[haut](#menu)
```{r premDerEc, echo=FALSE}
data <- data.frame(premder.ec())
data %>% filter(tisaep.all_2 != "")  %>% select(tisaep.all_2
,date.premder.all_2) 
```

## premder
[haut](#menu)
```{r premder, echo=FALSE}
data <- data.frame(premder.premder())
data %>% filter(tisaep.all_1 != "") %>% select(tisaep.all_1,date.premder.all_1,status.all_1) 
```

## prem der
[haut](#menu)
```{r premDer, echo=FALSE}
data <- data.frame(premder.prem.der())
data %>% filter(tisaep.all != "")  %>% select(tisaep.all
,date.premder.all,status.all) 
```

## nb
[haut](#menu)
```{r nb, echo=FALSE}
data <- data.frame(aListes())

# df <- data.frame(cbind(data$titre,data$nbDeLignes,data$titre_1,data$nbPage))
# df[1:50,]

data %>% filter(titre != "")  %>% select(titre,nbDeLignes) 
data %>% filter(titre_1 != "")  %>% select(titre_1,nbPage) 
```

## networks
[haut](#menu)
```{r network1, echo=FALSE}
### EXPLORE STRUCTURE 
max <- 10
df <- data.frame(listeCompl.ec())
df <- df[1:max,]

main <- function(){
  ### CREATE NODES TABLE 
get_random_names <- function(n) { 
  unique_names <- distinct(df, en.cours, .keep_all = TRUE) 
  index <- sample(1:nrow(unique_names), n, replace = FALSE) 
  names <- unique_names[index, ] 
  names 
}

nodes <- get_random_names(max)


### CREATE LINKS TABLE 
# Create source and target vectors 
src <- sample(1:nrow(nodes), nrow(nodes)*2, replace = TRUE)  
target <- sample(1:nrow(nodes), nrow(nodes)*2, replace = TRUE)

# Merge vectors to form a single table 
links <- data.frame(src, target)

# Clean up 
links <- data.frame(src, target) %>%  
  filter(!(src == target)) 
links <- unique(links[c("src", "target")])


### PLOT NETWORK 
# Type cast to tbl_graph object
social_net_tbls <- tbl_graph(nodes = nodes, 
                             edges = links, 
                             directed = FALSE)

# Create the network 
social_net <- ggraph(social_net_tbls, layout = "stress") +
  geom_node_point(size = 2) +                                         
  geom_node_text(aes(label = en.cours), nudge_y = 0.05, nudge_x = 0.2)+ 
  geom_edge_link() +
  theme_void()

# Render the network 
  show(social_net)
  # dev.print(device = png, file = "socialnet1.png", width = 600)
}
main()

```

## network2
```{r network2, echo=FALSE}
### EXPLORE STRUCTURE 
max <- 10
df <- data.frame(listeCompl.ec())
df <- df[1:max,]

main <- function(){
  ### CREATE NODES TABLE 
get_random_names <- function(n) { 
  unique_names <- distinct(df, en.cours, .keep_all = TRUE) 
  index <- sample(1:nrow(unique_names), n, replace = FALSE) 
  names <- unique_names[index, ] 
  names 
}

nodes <- get_random_names(max)


### CREATE LINKS TABLE 
# Create source and target vectors 
src <- sample(1:nrow(nodes), nrow(nodes)*2, replace = TRUE)  
target <- sample(1:nrow(nodes), nrow(nodes)*2, replace = TRUE)

# Merge vectors to form a single table 
links <- data.frame(src, target)

# Clean up 
links <- data.frame(src, target) %>%  
  filter(!(src == target)) 
links <- unique(links[c("src", "target")])


### PLOT NETWORK 
# Type cast to tbl_graph object
social_net_tbls <- tbl_graph(nodes = nodes, 
                             edges = links, 
                             directed = FALSE)

# Create the network 
social_net <- ggraph(social_net_tbls, layout = "stress") +
  geom_node_point(size = 2) +                                         
  geom_node_text(aes(label = en.cours), nudge_y = 0.05, nudge_x = 0.2)+ 
  geom_edge_link() +
  theme_void()

# Render the network 
  show(social_net)
  # dev.print(device = png, file = "socialnet1.png", width = 600)
}
main()

```
